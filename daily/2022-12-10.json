{"1670684894":[{"creator":"Liam J. Kelly","title":"This Week in Coins: Bitcoin and Ethereum Flat Again, Axie Infinity Sees Rare Jump","link":"https://decrypt.co/116853/this-week-in-coins-bitcoin-and-ethereum-flat-again-axie-infinity-sees-rare-jump","pubDate":"Sat, 10 Dec 2022 14:53:35 +0000","enclosure":{"url":"https://img.decrypt.co/insecure/rs:fill:1024:512:1:0/plain/https://cdn.decrypt.co/wp-content/uploads/2022/03/axie-infinity-origin-update-2022-gID_1.png@png","length":"1000000","type":"image/png"},"dc:creator":"Liam J. Kelly","content":"Bitcoin and Ethereum traded sideways all week, with the market offering only a few green shoots. Elsewhere, SBF looks set to testify on Tuesday.","contentSnippet":"Bitcoin and Ethereum traded sideways all week, with the market offering only a few green shoots. Elsewhere, SBF looks set to testify on Tuesday.","guid":"https://decrypt.co/?p=116853","categories":["axie-infinity","Coins"],"isoDate":"2022-12-10T14:53:35.000Z"},{"creator":"Jameson Lopp","title":"How to Archive & Transcribe YouTube Videos in Bulk","link":"https://blog.lopp.net/how-to-archive-transcribe-youtube-videos-bulk/","pubDate":"Sat, 10 Dec 2022 15:07:37 GMT","content:encoded":"<img src=\"https://blog.lopp.net/content/images/2022/12/monk_transcribing.png\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\"><p>Recently I decided to improve my sovereignty in a new way - by reducing my reliance upon third party hosting of the audio and video content I&apos;ve produced over the years.</p><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I grow weary of people informing me that YouTube videos I&apos;m in have been blackholed.<br><br>Going forward, I will archive everything myself. Turns out YouTube doesn&apos;t like it when you download over 10GB in a day... thankfully IP addresses are cheap!</p>&#x2014; Jameson Lopp (@lopp) <a href=\"https://twitter.com/lopp/status/1588917003112558593?ref_src=twsrc%5Etfw\">November 5, 2022</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n</figure><p>The following guide will walk you through the steps I took to accomplish this. It&apos;s a bunch of pretty technical Linux command line usage, so if you aren&apos;t fairly motivated to solve this problem for yourself, the rest of this article will be pretty boring.</p><p>The summary, for those who don&apos;t need to know the details, is that we&apos;ll leverage 2 tools along with some light scripting in order to make them process large batches of files with a single command. The first tool will download the video files from YouTube while the second will convert their audio into text.</p><p>On to the nitty gritty!</p><h3 id=\"backing-up-youtube-videos\">Backing Up YouTube Videos</h3><p>We&apos;ll be using <a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> to download the videos in bulk. If you&apos;re a pip user, I&apos;d just &quot;pip install yt-dlp&quot; but you can also simply download the standalone binaries from the project&apos;s github to which I linked. Note that yt-dlp is a <a href=\"https://github.com/ytdl-org/youtube-dl\">youtube-dl</a> fork based on the now inactive <a href=\"https://github.com/blackjack4494/yt-dlc\">youtube-dlc</a>. I started off using those older projects and quickly realized that yt-dlp is far more performant. Note that yt-dlp is a highly configurable tool with over a hundred options; for brevity I&apos;ll only be mentioning a few.</p><p><strong>Step 1</strong>: first you&apos;ll need to decide if you only want to save the videos or if you ALSO want to transcribe their audio into text files. If you want to skip the transcription steps then exclude the &quot;--extract-audio --audio-format mp3&quot; parameters from whichever command you decide to run.</p><p><strong>Step 2:</strong> get a list of all your YouTube video URLs you want to back up. You&apos;ll probably just be manually copying all of the URLs into a text file. Or, if you&apos;re OCD like myself and already have a <a href=\"https://www.lopp.net/interviews.html\">web page with all of the links on it</a>, you can use the following command to scrape them all:</p><!--kg-card-begin: markdown--><pre><code>curl -f -L -s https://url-to-page-with-links | grep -Eo &apos;https?://\\S+outu[^&quot;]+&apos; | xargs -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n</code></pre>\n<!--kg-card-end: markdown--><p>Otherwise, just feed your text file that contains one URL per line in it to yt-dlp:</p><!--kg-card-begin: markdown--><pre><code>cat your-file-of-urls.txt | xargs -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n</code></pre>\n<!--kg-card-end: markdown--><p>WARNING: if any of the URLs are &quot;youtube playlist&quot; URLs, it will download every single video on that playlist. You can pass &quot;--flat-playlist &quot; to yt-dlp prevent that. I&apos;d also suggest using &quot;--restrict-filenames&quot; to prevent weird stuff like emojis showing up in your file names, which can cause issues with later processing steps.</p><p>Alternatively, if you have (or create) a YouTube playlist for which you want to grab all the videos listed on it just use:</p><!--kg-card-begin: markdown--><pre><code>yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 --yes-playlist &lt;playlist-url&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Step 3</strong>: wait. This may take a very long time to run depending upon how many videos you have and their total length. If it looks like YouTube is throttling your download speed, you can work around that by running multiple yt-dlp jobs in parallel by tweaking xargs parameters:</p><!--kg-card-begin: markdown--><pre><code>cat your-file-of-urls.txt | xargs -P 10 -n 1 -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n</code></pre>\n<!--kg-card-end: markdown--><p>Note that the &quot;-P 10 -n 1&quot; parameters are what really speeds it up and tells it to run 10 processes in parallel.</p><h3 id=\"potential-problems\">Potential Problems</h3><p>Unfortunately, YouTube doesn&apos;t take kindly to having a single IP address eat up far more bandwidth than it should for simple streaming. If you&apos;re lazy or patient then you can just run the command in serial without the &quot;-P 10 -n 1&quot; arguments. If you&apos;re in more of a hurry, I recommend using a VPN so that you can easily change IP addresses if they ban you. From my testing YouTube starts returning 403 (forbidden) errors after about 20 GB is downloaded in a single day. The nice thing is that the yt-dlp command will gracefully resume partial downloads, so it&apos;s not a big deal to switch IPs and re-run the original batch command.</p><h3 id=\"preserving-your-backups\">Preserving Your Backups</h3><p>If you don&apos;t care about transcribing your videos, now you&apos;re almost done! The last step will be for you to decide how to back up your archive of video files so that you&apos;re confident they won&apos;t be lost if they are removed from YouTube. Of course there are innumerable ways to do backups; I&apos;d suggest checking out my guide for creating secure cloud backups here:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://blog.lopp.net/how-to-securely-back-up-data-to-cloud-storage/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">How to Securely Back Up Data to Cloud Storage</div><div class=\"kg-bookmark-description\">Jameson researches user-friendly solutions for backing up sensitive data to cloud providers while keeping it safe from third party snooping.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://blog.lopp.net/favicon.ico\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\"><span class=\"kg-bookmark-author\">Cypherpunk Cogitations</span><span class=\"kg-bookmark-publisher\">Jameson Lopp</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://blog.lopp.net/content/images/2020/01/cloud-3461414_960_720.png\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\"></div></a></figure><h3 id=\"transcribing-in-bulk\">Transcribing in Bulk</h3><p>If you&apos;re lucky, you&apos;ll also be able to grab subtitle files for the videos by passing the &quot;--write-subs&quot; parameter to yt-dlp. If a video doesn&apos;t have any subtitles, you&apos;ll want to feed the mp3 through Whisper to transcribe it. In my case, I&apos;m going to use Whisper for everything because I suspect it will result in a more accurate transcription. My recent Whisper testing of performance and accuracy post can be found here:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://blog.lopp.net/openai-whisper-transcription-testing/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">OpenAI Whisper Transcription Testing</div><div class=\"kg-bookmark-description\">Accuracy and performance testing of OpenAI&#x2019;s transcription software.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://blog.lopp.net/favicon.ico\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\"><span class=\"kg-bookmark-author\">Cypherpunk Cogitations</span><span class=\"kg-bookmark-publisher\">Jameson Lopp</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://blog.lopp.net/content/images/2022/09/transcription_robot.png\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\"></div></a></figure><p>As my previous article noted, it&apos;s <em>highly recommended</em> to run Whisper on a CUDA compatible GPU - it will be ~50X faster than just using your CPU. I ran my initial tests for the previous article on a GPU focused VPS running Ubuntu and found it was pretty easy to get set up with the appropriate drivers. This time around I decided to cheap out and use a Windows gaming machine I had on hand. I&apos;ll note that it was <em>far more difficult</em> for me to get Whisper working on Windows with CUDA - it doesn&apos;t seem to be as well supported. I ended up having to manually downgrade my python version and manually change the version of the python torch library, which took me several hours of internet sleuthing to figure out was the solution to my problems.</p><p>Eventually I found that my Nvidia GeForce RTX 2080Ti is able to transcribe over 5 hours of audio in 1 hour on the medium quality model. Thus I was able to transcribe my 100+ hours of content in about a day.</p><p>All you need to do is gather your audio files (mp3 / m4a / wav) and put them all in the same directory. Then with one batch command we can iterate over them with Whisper.</p><p>Bulk Windows (powershell) processing:</p><!--kg-card-begin: markdown--><pre><code>Get-ChildItem -Recurse -Include &quot;.mp3&quot;,&quot;.m4a&quot;,&quot;*.wav&quot; D:\\path\\to\\audio\\files\\ | % {$outfile = Join-Path $.DirectoryName ($.BaseName + &apos;.txt&apos;)&amp; whisper.exe --model medium --language English --device cuda $_.FullName &gt; $outfile}\n</code></pre>\n<!--kg-card-end: markdown--><p>Bulk Linux (bash) processing:</p><!--kg-card-begin: markdown--><pre><code>find /path/to/audio/files -type f | egrep &quot;[.]mp3|[.]m4a|[.]wav&quot; | xargs -I % sh -c &quot;whisper --model medium --language English --device cuda % &gt; %.txt&quot;\n</code></pre>\n<!--kg-card-end: markdown--><p>This will give us a bunch of text files with the raw output from Whisper. Unfortunately, there doesn&apos;t appear to be an option to tell Whisper to skip printing out the timestamps for each segment of transcribed text. Thankfully, they are so well-formatted that with a few more commands we can strip out the timestamps.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://blog.lopp.net/content/images/2022/12/transcript.png\" class=\"kg-image\" alt=\"How to Archive &amp; Transcribe YouTube Videos in Bulk\" loading=\"lazy\" width=\"1160\" height=\"584\" srcset=\"https://blog.lopp.net/content/images/size/w600/2022/12/transcript.png 600w, https://blog.lopp.net/content/images/size/w1000/2022/12/transcript.png 1000w, https://blog.lopp.net/content/images/2022/12/transcript.png 1160w\"></figure><p>First, let&apos;s remove any whitespace from the file names to make the later commands simpler. You can do that in bash on Linux with this one-liner:</p><pre><code class=\"language-bash\">for f in *\\ *; do mv &quot;$f&quot; &quot;${f// /_}&quot;; done</code></pre><p>Next we need to strip an extraneous 2 bytes off the beginning of each file. I&apos;m not sure why Whisper is outputting those bytes... maybe it was because I ran mine on a Windows machine and then am doing the post-processing on a Linux machine. We need 2 commands to do this because &quot;tail&quot; doesn&apos;t support in-place file rewriting.</p><pre><code class=\"language-bash\">find . -name &quot;*.txt&quot; -type f -exec sh -c &apos;f={}; tail -c +3 $f &gt; $f.tmp&apos; \\;\nfind . -name &quot;*.tmp&quot; -type f -exec sh -c &apos;f={}; mv $f ${f%.*}&apos; \\;</code></pre><p>Now we&apos;re ready to actually strip out the timestamp prefixes from each line, which the following command will accomplish:</p><pre><code class=\"language-bash\">find . -name &quot;*.txt&quot; -type f -print0 | xargs -0 sed -i -r &apos;s/^.+]....//g&apos;</code></pre><p>One thing that I found quite helpful is that it appears Whisper is (sometimes) able to differentiate between different voices and it separates their text into different paragraphs. This saved me a ton of time to make my interviews more transcript-like.</p><p>After this point I just did some light editing of the transcripts, mostly doing find and replace on people&apos;s names, which Whisper can have a hard time with.</p><h3 id=\"finis\">Finis</h3><p>That&apos;s all there is to it! Hopefully the time I spent figuring out all of the above commands will save some of you a lot of effort in following my footsteps.</p><p>If you have any issues following this guide, or happen to be a Windows powershell expert who can contribute powershell versions of my bash commands, don&apos;t hesitate to <a href=\"https://www.lopp.net/contact\">contact me</a>!</p>","content:encodedSnippet":"Recently I decided to improve my sovereignty in a new way - by reducing my reliance upon third party hosting of the audio and video content I've produced over the years.\n\nI grow weary of people informing me that YouTube videos I'm in have been blackholed.\nGoing forward, I will archive everything myself. Turns out YouTube doesn't like it when you download over 10GB in a day... thankfully IP addresses are cheap!\n— Jameson Lopp (@lopp) November 5, 2022\n\n\nThe following guide will walk you through the steps I took to accomplish this. It's a bunch of pretty technical Linux command line usage, so if you aren't fairly motivated to solve this problem for yourself, the rest of this article will be pretty boring.\nThe summary, for those who don't need to know the details, is that we'll leverage 2 tools along with some light scripting in order to make them process large batches of files with a single command. The first tool will download the video files from YouTube while the second will convert their audio into text.\nOn to the nitty gritty!\nBacking Up YouTube Videos\nWe'll be using yt-dlp to download the videos in bulk. If you're a pip user, I'd just \"pip install yt-dlp\" but you can also simply download the standalone binaries from the project's github to which I linked. Note that yt-dlp is a youtube-dl fork based on the now inactive youtube-dlc. I started off using those older projects and quickly realized that yt-dlp is far more performant. Note that yt-dlp is a highly configurable tool with over a hundred options; for brevity I'll only be mentioning a few.\nStep 1: first you'll need to decide if you only want to save the videos or if you ALSO want to transcribe their audio into text files. If you want to skip the transcription steps then exclude the \"--extract-audio --audio-format mp3\" parameters from whichever command you decide to run.\nStep 2: get a list of all your YouTube video URLs you want to back up. You'll probably just be manually copying all of the URLs into a text file. Or, if you're OCD like myself and already have a web page with all of the links on it, you can use the following command to scrape them all:\n\ncurl -f -L -s https://url-to-page-with-links | grep -Eo 'https?://\\S+outu[^\"]+' | xargs -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n\nOtherwise, just feed your text file that contains one URL per line in it to yt-dlp:\n\ncat your-file-of-urls.txt | xargs -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n\nWARNING: if any of the URLs are \"youtube playlist\" URLs, it will download every single video on that playlist. You can pass \"--flat-playlist \" to yt-dlp prevent that. I'd also suggest using \"--restrict-filenames\" to prevent weird stuff like emojis showing up in your file names, which can cause issues with later processing steps.\nAlternatively, if you have (or create) a YouTube playlist for which you want to grab all the videos listed on it just use:\n\nyt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 --yes-playlist <playlist-url>\n\nStep 3: wait. This may take a very long time to run depending upon how many videos you have and their total length. If it looks like YouTube is throttling your download speed, you can work around that by running multiple yt-dlp jobs in parallel by tweaking xargs parameters:\n\ncat your-file-of-urls.txt | xargs -P 10 -n 1 -L 1 -I thisurl yt-dlp -k --write-auto-sub --extract-audio --audio-format mp3 thisurl\n\nNote that the \"-P 10 -n 1\" parameters are what really speeds it up and tells it to run 10 processes in parallel.\nPotential Problems\nUnfortunately, YouTube doesn't take kindly to having a single IP address eat up far more bandwidth than it should for simple streaming. If you're lazy or patient then you can just run the command in serial without the \"-P 10 -n 1\" arguments. If you're in more of a hurry, I recommend using a VPN so that you can easily change IP addresses if they ban you. From my testing YouTube starts returning 403 (forbidden) errors after about 20 GB is downloaded in a single day. The nice thing is that the yt-dlp command will gracefully resume partial downloads, so it's not a big deal to switch IPs and re-run the original batch command.\nPreserving Your Backups\nIf you don't care about transcribing your videos, now you're almost done! The last step will be for you to decide how to back up your archive of video files so that you're confident they won't be lost if they are removed from YouTube. Of course there are innumerable ways to do backups; I'd suggest checking out my guide for creating secure cloud backups here:\n\nHow to Securely Back Up Data to Cloud Storage\nJameson researches user-friendly solutions for backing up sensitive data to cloud providers while keeping it safe from third party snooping.\nCypherpunk CogitationsJameson Lopp\n\n\n\nTranscribing in Bulk\nIf you're lucky, you'll also be able to grab subtitle files for the videos by passing the \"--write-subs\" parameter to yt-dlp. If a video doesn't have any subtitles, you'll want to feed the mp3 through Whisper to transcribe it. In my case, I'm going to use Whisper for everything because I suspect it will result in a more accurate transcription. My recent Whisper testing of performance and accuracy post can be found here:\n\nOpenAI Whisper Transcription Testing\nAccuracy and performance testing of OpenAI’s transcription software.\nCypherpunk CogitationsJameson Lopp\n\n\n\nAs my previous article noted, it's highly recommended to run Whisper on a CUDA compatible GPU - it will be ~50X faster than just using your CPU. I ran my initial tests for the previous article on a GPU focused VPS running Ubuntu and found it was pretty easy to get set up with the appropriate drivers. This time around I decided to cheap out and use a Windows gaming machine I had on hand. I'll note that it was far more difficult for me to get Whisper working on Windows with CUDA - it doesn't seem to be as well supported. I ended up having to manually downgrade my python version and manually change the version of the python torch library, which took me several hours of internet sleuthing to figure out was the solution to my problems.\nEventually I found that my Nvidia GeForce RTX 2080Ti is able to transcribe over 5 hours of audio in 1 hour on the medium quality model. Thus I was able to transcribe my 100+ hours of content in about a day.\nAll you need to do is gather your audio files (mp3 / m4a / wav) and put them all in the same directory. Then with one batch command we can iterate over them with Whisper.\nBulk Windows (powershell) processing:\n\nGet-ChildItem -Recurse -Include \".mp3\",\".m4a\",\"*.wav\" D:\\path\\to\\audio\\files\\ | % {$outfile = Join-Path $.DirectoryName ($.BaseName + '.txt')& whisper.exe --model medium --language English --device cuda $_.FullName > $outfile}\n\nBulk Linux (bash) processing:\n\nfind /path/to/audio/files -type f | egrep \"[.]mp3|[.]m4a|[.]wav\" | xargs -I % sh -c \"whisper --model medium --language English --device cuda % > %.txt\"\n\nThis will give us a bunch of text files with the raw output from Whisper. Unfortunately, there doesn't appear to be an option to tell Whisper to skip printing out the timestamps for each segment of transcribed text. Thankfully, they are so well-formatted that with a few more commands we can strip out the timestamps.\n\nFirst, let's remove any whitespace from the file names to make the later commands simpler. You can do that in bash on Linux with this one-liner:\nfor f in *\\ *; do mv \"$f\" \"${f// /_}\"; done\nNext we need to strip an extraneous 2 bytes off the beginning of each file. I'm not sure why Whisper is outputting those bytes... maybe it was because I ran mine on a Windows machine and then am doing the post-processing on a Linux machine. We need 2 commands to do this because \"tail\" doesn't support in-place file rewriting.\nfind . -name \"*.txt\" -type f -exec sh -c 'f={}; tail -c +3 $f > $f.tmp' \\;\nfind . -name \"*.tmp\" -type f -exec sh -c 'f={}; mv $f ${f%.*}' \\;\nNow we're ready to actually strip out the timestamp prefixes from each line, which the following command will accomplish:\nfind . -name \"*.txt\" -type f -print0 | xargs -0 sed -i -r 's/^.+]....//g'\nOne thing that I found quite helpful is that it appears Whisper is (sometimes) able to differentiate between different voices and it separates their text into different paragraphs. This saved me a ton of time to make my interviews more transcript-like.\nAfter this point I just did some light editing of the transcripts, mostly doing find and replace on people's names, which Whisper can have a hard time with.\nFinis\nThat's all there is to it! Hopefully the time I spent figuring out all of the above commands will save some of you a lot of effort in following my footsteps.\nIf you have any issues following this guide, or happen to be a Windows powershell expert who can contribute powershell versions of my bash commands, don't hesitate to contact me!","dc:creator":"Jameson Lopp","content":"A step-by-step guide for leveraging tools to download and transcribe videos in bulk with just a few commands.","contentSnippet":"A step-by-step guide for leveraging tools to download and transcribe videos in bulk with just a few commands.","guid":"6361a561decba4a2cdd626b9","categories":["backups","ai"],"isoDate":"2022-12-10T15:07:37.000Z"}]}